{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "from itertools import islice\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import random\n",
    "import concurrent.futures\n",
    "import torch\n",
    "import torchaudio\n",
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_AudioCommon import *\n",
    "from exp.nb_DataBlock import *\n",
    "from exp.nb_DataAugmentation import *\n",
    "from exp.nb_TransformsManager import *\n",
    "from exp.nb_AudioTransformsManager import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/jupyter/rob/TIMIT/timit\")\n",
    "path.ls()\n",
    "path_test = Path(\"/home/jupyter/rob/test_augment/audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_phoneme = path/\"PHONEMES\"\n",
    "path_spectrogram = path/\"spectrogram\"\n",
    "path_synth = path/\"synth\"\n",
    "path_google = Path('/home/jupyter/rob/googlespeech/train/audio3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_maps = {\n",
    "    'aa': 'ɑ', 'ae':'æ', 'ah':'ʌ', 'ao':'ɔ', 'aw':'aʊ', 'ax':'ə',\n",
    "    'axr':'ɚ', 'ay':'aɪ', 'eh':'ɛ', 'er':'ɝ', 'ey':'eɪ', 'ih':'ɪ',\n",
    "    'ix':'ɪ', 'iy':'i', 'ow':'oʊ', 'oy':'ɔɪ', 'uh':'ʊ', 'uw':'u', 'ux':'u',\n",
    "}\n",
    "\n",
    "#dx is the flap like tt in butter, arpabet says it translates to ɾ in ipa\n",
    "#but im not so sure\n",
    "#nx is another one to be careful with, it translates to either ng or n as in winner\n",
    "#wh is meant to be wh like why/when/where but most ipa consider it a w\n",
    "cons_maps = {\n",
    "    'ch':'tʃ', 'dh':'ð', 'dx':'ɾ', 'el':'l', 'em':'m', 'en':'n', 'hh':'h',\n",
    "    'jh':'dʒ', 'ng':'ŋ', 'nx':'n', 'q':'ʔ', 'r':'ɹ', 'sh':'ʃ', 'th':'θ',\n",
    "    'wh':'w', 'y':'j', 'zh':'ʒ'\n",
    "}\n",
    "\n",
    "#these are maps that only timit uses, not arpanet\n",
    "timit_specific_maps = {\n",
    "    'ax-h':'ə', 'bcl':'b', 'dcl':'d', 'eng':'ŋ', 'gcl':'g', 'hv':'h', 'kcl':'k',\n",
    "    'pcl':'p', 'tcl':'t', 'pau':'silence', 'epi':'silence', 'h#':'silence',\n",
    "}\n",
    "\n",
    "def get_timit_to_ipa_dict():\n",
    "    timit_phonemes = [x.stem for x in path_phoneme.ls()]\n",
    "    timit_to_ipa_dict = {k:k for k in timit_phonemes}\n",
    "    for k,v in vowel_maps.items(): timit_to_ipa_dict[k] = v\n",
    "    for k,v in cons_maps.items(): timit_to_ipa_dict[k] = v\n",
    "    for k,v in timit_specific_maps.items(): timit_to_ipa_dict[k] = v\n",
    "    return timit_to_ipa_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timit_dict():\n",
    "    timit_dict = {}\n",
    "    with open(path/\"DOC/TIMITDIC.TXT\") as f:\n",
    "        for line in f:\n",
    "            if line[0].isalpha():\n",
    "                #note the split is by double space, not single\n",
    "                word, timit_string = line.split('  ')\n",
    "                timit_string = timit_string.replace('/', '').replace('1', '').replace('2','').strip()\n",
    "                timit_dict[word] = timit_string\n",
    "    return timit_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_string_dict = get_timit_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_string_dict[\"zoologist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timit_to_ipa_dict = get_timit_to_ipa_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipa_to_timit_dict = {v:k for k,v in timit_to_ipa_dict.items() if 'cl' not in k and k not in ['wh','nx', 'en', 'em']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths_by_phoneme():\n",
    "    len_dict = {}\n",
    "    count = 0\n",
    "    for p in path_phoneme.ls():\n",
    "        len_dict[p] = []\n",
    "        print(p)\n",
    "        p_count = 0\n",
    "        p_len = 0\n",
    "        with os.scandir(p) as sd:\n",
    "            for entry in sd:\n",
    "                if(count % 10000 == 9999):\n",
    "                    print(count)\n",
    "                count+=1\n",
    "                fname = path/p/entry\n",
    "                y, sr = librosa.load(fname)\n",
    "                len_dict[p].append(len(y)/sr)\n",
    "    return len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_speaker_ids():\n",
    "    speaker_list = []\n",
    "    for p in path_phoneme.ls():\n",
    "        with os.scandir(p) as sd:\n",
    "            for entry in sd:\n",
    "                speaker_id = str(entry).split('-')[1]\n",
    "                speaker_list.append(speaker_id)\n",
    "    return list(set(speaker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = get_all_speaker_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a few minutes to run, generates a dict with a key for each phoneme and value is list of the lengths \n",
    "# of each sample we have of that phoneme. Used to generate stats. \n",
    "# len_dict = get_lengths_by_phoneme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed_dict = {str(k).split('/')[-1]:v for k,v in len_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(fixed_dict, open( \"phon_len.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dict = pickle.load(open(\"phon_len.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rd4(x):\n",
    "    return round(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict = {k:list(map(rd4, [min(v), max(v), sum(v)/len(v)])) for k,v in len_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ipa_to_timit(ipa_string):\n",
    "    timit_list = []\n",
    "    skip=False\n",
    "    ipa_string = ipa_string.replace('a', 'ɑ').replace('r', 'ɹ')\n",
    "    for i in range(len(ipa_string)):\n",
    "        if(skip):\n",
    "            skip = False\n",
    "            continue\n",
    "        if(i < len(ipa_string)-1) and ipa_string[i:i+2] in ipa_to_timit_dict:\n",
    "            timit_list.append(ipa_to_timit_dict[ipa_string[i:i+2]])\n",
    "            skip = True\n",
    "        else: timit_list.append(ipa_to_timit_dict[ipa_string[i]])\n",
    "    return timit_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_ipa_to_timit('jɛs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_ipa_to_timit('daʊn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_ipa_to_timit('dɔg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_word(word, timit_string_dict = None, speaker_ids=None, by_sex = True, above_mean = True, one_speaker=False):\n",
    "    timit_string_dict = timit_string_dict or get_timit_dict()\n",
    "    speaker_ids = speaker_ids or get_all_speaker_ids()\n",
    "    if(word == \"bird\"): timit_string = \"b er d\"\n",
    "    else: timit_string = timit_string_dict[word]\n",
    "    print(timit_string)\n",
    "    timit_list = timit_string.split()\n",
    "    speaker_sex_list = random.choice([['M'], ['F']]) if by_sex else ['M', 'F']\n",
    "    \n",
    "    speaker_id_list = [random.choice(speaker_ids)] if one_speaker else speaker_ids\n",
    "    tensor_list = []\n",
    "    for timit_phoneme in timit_list:\n",
    "        fnames = [fname for fname in os.listdir(path_phoneme/timit_phoneme) if fname[4] in speaker_sex_list  and fname[4:9] in speaker_id_list]\n",
    "        fname = random.choice(fnames)\n",
    "        y, sr = torchaudio.load(path_phoneme/timit_phoneme/fname)\n",
    "        if(above_mean):\n",
    "            while(len(y[0])/sr < stats_dict[timit_phoneme][2]):\n",
    "                fname = random.choice(fnames)\n",
    "                y, sr = torchaudio.load(path_phoneme/timit_phoneme/fname)\n",
    "        tensor_list.append(y)\n",
    "    return torch.cat(tensor_list, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = synthesize_word(\"down\", above_mean=True, one_speaker=False)\n",
    "display(AudioItem(AudioData(x, 16000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_commands = \"bed bird dog down no off on one three tree\".split()\n",
    "ipa_commands = 'bɛd bɝrd dɔg daʊn noʊ ɔf ɑn wʌn θri tri'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for word in eng_commands:\n",
    "    print(word)\n",
    "    x = synthesize_word(word)\n",
    "    display(AudioItem(AudioData(x, 16000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: come back here and make sure we are only grabbing phonemes of a certain length when constructing words\n",
    "# Use the mean length of each indiv. one as a guide, and then make a huge dataset. (can also consider fine tuning \n",
    "# by gender or individual speaker, look at the stats and see how this would affect the combinatorics)\n",
    "def synthesize_timit_word_from_ipa(ipa_string):\n",
    "    tensor_list = []\n",
    "    timit_list = convert_ipa_to_timit(ipa_string)\n",
    "    for timit_phoneme in timit_list:\n",
    "        fname = random.choice((path_phoneme/timit_phoneme).ls())\n",
    "        y, sr = torchaudio.load(fname)\n",
    "        tensor_list.append(y)\n",
    "    return torch.cat(tensor_list, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_n_examples_of_word(n, word, by_sex = True, above_mean = True, one_speaker=False):\n",
    "    timit_string_dict = get_timit_dict()\n",
    "    speaker_ids = get_all_speaker_ids()\n",
    "    if not(os.path.exists(path_synth/word)): os.mkdir(path_synth/word)\n",
    "    for i in range(n):\n",
    "        x = synthesize_word(word, timit_string_dict=timit_string_dict, speaker_ids=speaker_ids,\n",
    "                            by_sex=by_sex, above_mean=above_mean, one_speaker=one_speaker)\n",
    "        torchaudio.save(f'{path_command}/{word}-{i}.wav', x, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in eng_commands:\n",
    "    synthesize_n_examples_of_word(5000, word, by_sex=True, above_mean=True, one_speaker=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list_train = AudioList.from_folder(path_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_google.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list_valid = AudioList.from_folder(path_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list_train.add(audio_list_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list = audio_list_train.split_by_valid_func(valid_func).label_from_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_func(o):\n",
    "    return 'nohash' in o.stem and o.stem[3] in '2 4 6 8'.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio_list.add_test_folder(path_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_speech = AudioTfmsManager.get_audio_tfms_manager(\n",
    "                            spec_augment=True, pct_hori=.2, num_vert=0, num_hori=1,\n",
    "                            spectro=True, #We're going to replace it...\n",
    "                            mx_to_pad=16127, pad_type=\"middle\", #1 sec window\n",
    "                            white_noise=False, noise_scl=0.001, # Small noise\n",
    "                            modulate_volume=False, lower_gain=.95, upper_gain=1.05, # Not big volume variation\n",
    "                            random_cutout=False,\n",
    "                            pad_with_silence=False,\n",
    "                            pitch_warp=False,\n",
    "                            down_and_up=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tm_speech.get_tfms()\n",
    "del tfms[1][-1]\n",
    "tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_speech = audio_list.transform(tfms).databunch(bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_first_layer(src_model, nChannels):\n",
    "    '''\n",
    "    Change first layer of network to accomodate new channels\n",
    "    '''\n",
    "    # save original\n",
    "    original_weights = src_model[0][0].weight.clone()\n",
    "    new_weights = original_weights[:,0:1,:,:]\n",
    "\n",
    "    # create new layes\n",
    "    new_layer = nn.Conv2d(nChannels,64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    new_layer.weight = nn.Parameter(new_weights)\n",
    "\n",
    "    # Replace layer and put to gpu.\n",
    "    src_model[0][0] = new_layer\n",
    "    src_model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_speech.train_ds[0][0].show()\n",
    "data_speech.valid_ds[0][0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_speech = cnn_learner(data_speech, models.resnet50, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nChannels=1\n",
    "\n",
    "# Alter existing model\n",
    "adapt_first_layer(learn_speech.model,nChannels)\n",
    "#print(f'First layer shape: {learn_speech.model[0][0].weight.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_speech.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_speech.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_speech.fit_one_cycle(8, 2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_speech.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_speech.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_speech.fit_one_cycle(9, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_speech.save(\"synth-99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_speech.load(\"synth-stage2\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_speech.test_ds[1300][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for d in data_speech.test_ds[0:250]:\n",
    "    if learn_speech.predict(d[0])[1].item() == 5:\n",
    "        correct+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn_speech.predict(data_speech.test_ds[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_list.test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_audio_prediction():\n",
    "    rand_file = test_files_list[random.randint(0, num_files-1)]\n",
    "    clip, sr = librosa.load(path_test_audio/rand_file, sr=None)\n",
    "    print(rand_file)\n",
    "    img_filename = rand_file + \".png\"\n",
    "    image = open_image(path_test_spectrogram/img_filename)\n",
    "    pred = learn.predict(image)\n",
    "    print(f\"Prediction: {pred[0]}\")\n",
    "    for idx, pct in enumerate(pred[2]):\n",
    "        if(pct.item() > 0.1):\n",
    "            print(f\"{data.classes[idx]}: {round(pct.item()*100, 2)}%\")\n",
    "    display(Audio(clip, rate=sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
